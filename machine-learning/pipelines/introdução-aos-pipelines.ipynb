{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos supor que temos um conjunto de dados sobre preços de casas. Este conjunto tem características como o tamanho da casa (em $m^2$), o número de quartos e o bairro em que a casa está localizada entre outras. O conjunto de dados \"Boston Housing\" é um conjunto de dados amplamente usado em regressão no campo do aprendizado de máquina. Ele contém informações sobre várias casas em Boston e é frequentemente usado para prever o valor médio das casas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
      "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
      "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
      "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
      "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  MEDV  \n",
      "0     15.3  396.90   4.98  24.0  \n",
      "1     17.8  396.90   9.14  21.6  \n",
      "2     17.8  392.83   4.03  34.7  \n",
      "3     18.7  394.63   2.94  33.4  \n",
      "4     18.7  396.90   5.33  36.2  \n"
     ]
    }
   ],
   "source": [
    "# URL do conjunto de dados Boston Housing no repositório UCI\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\"\n",
    "\n",
    "# Nomes das colunas (características)\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "\n",
    "# Baixando o conjunto de dados e ler para um DataFrame do pandas\n",
    "boston_df = pd.read_csv(url, delim_whitespace=True, header=None, names=column_names)\n",
    "\n",
    "# Verificando as primeiras linhas\n",
    "print(boston_df.head())\n",
    "\n",
    "# Dividiindo o DataFrame em características (X) e alvo (y) conforme necessário:\n",
    "X = boston_df.drop('MEDV', axis=1)\n",
    "y = boston_df['MEDV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Coluna | Descrição |\n",
    "|--------|-----------|\n",
    "| `CRIM` | Taxa de criminalidade per capita por cidade. |\n",
    "| `ZN` | Proporção de terrenos residenciais zoneados para lotes com mais de 25.000 sq.ft. |\n",
    "| `INDUS` | Proporção de acres comerciais não varejistas por cidade. |\n",
    "| `CHAS` | Variável fictícia Charles River (1 se o trecho limita o rio; 0 caso contrário). |\n",
    "| `NOX` | Concentração de óxidos nítricos (partes por 10 milhões). |\n",
    "| `RM` | Número médio de quartos por habitação. |\n",
    "| `AGE` | Proporção de unidades ocupadas pelo proprietário construídas antes de 1940. |\n",
    "| `DIS` | Distâncias ponderadas até cinco centros de emprego de Boston. |\n",
    "| `RAD` | Índice de acessibilidade às rodovias radiais. |\n",
    "| `TAX` | Taxa de imposto sobre a propriedade de valor total por $10,000. |\n",
    "| `PTRATIO` | Proporção aluno-professor por cidade. |\n",
    "| `B` | $1000(Bk - 0.63)^2$ onde Bk é a proporção de pessoas de descendência afro-americana por cidade. |\n",
    "| `LSTAT` | % menor status da população. |\n",
    "| `MEDV` | Valor mediano de casas ocupadas pelo proprietário em $1000s. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. O tamanho da casa precisa ser normalizado porque os modelos geralmente funcionam melhor com características numéricas em escalas semelhantes.\n",
    "\n",
    "2. O bairro é uma característica categórica e precisa ser codificado para ser utilizado por algoritmos de aprendizado de máquina.\n",
    "\n",
    "3. Podemos ter alguns valores ausentes que precisam ser tratados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução sem Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raiz do Erro Quadrático Médio (RMSE): 4.93\n"
     ]
    }
   ],
   "source": [
    "# Os dados foram carregados lá em cima\n",
    "\n",
    "# Divida os dados em conjuntos de treinamento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Pré-processe os dados (normalização)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Treine o modelo de regressão linear\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Faça as previsões e avalie o modelo\n",
    "y_pred = regressor.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solução com Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raiz do Erro Quadrático Médio (RMSE): 4.93\n"
     ]
    }
   ],
   "source": [
    "# Os dados foram carregados lá em cima\n",
    "\n",
    "# Com um pipeline, você pode tratar as operações como um único bloco\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Etapa de pré-processamento\n",
    "    ('regressor', LinearRegression()) # Etapa de modelagem\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Fazendo predições e avaliando o desempenho\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mesmo em um caso simples, podemos notar algumas vantagens do uso de um pipeline. \n",
    "\n",
    "* A preocupação em padronizar o conjunto apenas depois da divisão some com o uso do pipeline. \n",
    "\n",
    "* Detalhes de implementação do `sklearn`, como por exemplo utilizar o `fit_transform()` a primeira vez e em seguida lembrar de usar o `fit()` também desaparece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos aprender a usar a ferramenta com alguns exemplos práticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios Práticos (com respostas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex01: Pipeline Básico\n",
    "Crie um pipeline básico que apenas padroniza os dados e aplica uma regressão linear. Use o conjunto de dados Boston Housing para treinar e testar o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "<!--\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Adicionando SimpleImputer ao pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}\")\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SimpleImputer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `SimpleImputer` é uma classe de pré-processamento usada para preencher valores ausentes nos dados. Dependendo do argumento `strategy` fornecido, ele pode preencher os valores ausentes com a média (para colunas numéricas), a mediana (para colunas numéricas), o valor mais frequente (para colunas categóricas) ou até mesmo um valor constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idade  salario departamento\n",
      "0   25.0  50000.0           RH\n",
      "1   30.0  55000.0   Engenharia\n",
      "2   35.0      NaN   Engenharia\n",
      "3    NaN  65000.0           RH\n",
      "4   50.0  70000.0          NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'idade': [25, 30, 35, np.nan, 50],\n",
    "    'salario': [50000, 55000, np.nan, 65000, 70000],\n",
    "    'departamento': ['RH', 'Engenharia', 'Engenharia', 'RH', np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este DataFrame tem valores ausentes nas colunas 'idade', 'salario' e 'departamento'.\n",
    "\n",
    "Vamos usar o `SimpleImputer` para tratar esses valores ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   idade  salario departamento\n",
      "0   25.0  50000.0           RH\n",
      "1   30.0  55000.0   Engenharia\n",
      "2   35.0  60000.0   Engenharia\n",
      "3   35.0  65000.0           RH\n",
      "4   50.0  70000.0   Engenharia\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preenchendo os valores ausentes na coluna 'idade' com a média\n",
    "imputer_idade = SimpleImputer(strategy='mean')\n",
    "df['idade'] = imputer_idade.fit_transform(df[['idade']])\n",
    "\n",
    "# Preenchendo os valores ausentes na coluna 'salario' com a mediana\n",
    "imputer_salario = SimpleImputer(strategy='median')\n",
    "df['salario'] = imputer_salario.fit_transform(df[['salario']])\n",
    "\n",
    "# Preenchendo os valores ausentes na coluna 'departamento' com o valor mais frequente\n",
    "imputer_departamento = SimpleImputer(strategy='most_frequent')\n",
    "df['departamento'] = imputer_departamento.fit_transform(df[['departamento']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex02: Adicionando Etapas de Pré-processamento\n",
    "Estenda o pipeline do Exercício 1 para lidar com dados ausentes usando o `SimpleImputer` e, em seguida, aplique a padronização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua solução aqui\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Adicionando SimpleImputer ao pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "\n",
    "<!--\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Adicionando SimpleImputer ao pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Raiz do Erro Quadrático Médio (RMSE): {rmse:.2f}\")\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `ColumnTransformer` permite aplicar transformadores a colunas específicas de um DataFrame ou matriz de dados. Ele é útil quando diferentes colunas ou tipos de características requerem diferentes pré-processamentos.\n",
    "\n",
    "Muitos conjuntos de dados contêm características que têm tipos diferentes e que exigem diferentes transformações de pré-processamento. Por exemplo, um conjunto de dados pode ter características numéricas que precisam ser padronizadas e características categóricas que precisam ser codificadas em um formato numérico (como através de codificação one-hot). O `ColumnTransformer` facilita a aplicação dessas transformações diferentes às colunas relevantes de maneira organizada e integrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idade</th>\n",
       "      <th>salario</th>\n",
       "      <th>departamento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>RH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>55000</td>\n",
       "      <td>Engenharia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idade  salario departamento\n",
       "0     25    50000           RH\n",
       "1     30    55000   Engenharia"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'idade': [25, 30, 35, 40, 50],\n",
    "    'salario': [50000, 55000, 60000, 65000, 70000],\n",
    "    'departamento': ['RH', 'Engenharia', 'Engenharia', 'RH', 'Contabilidade']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desejamos padronizar as colunas numéricas (`idade` e `salario`) e aplicar uma codificação one-hot na coluna categórica (`departamento`). Podemos fazer isso com o `ColumnTransformer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.27872403 -1.41421356  0.          0.          1.        ]\n",
      " [-0.69748583 -0.70710678  0.          1.          0.        ]\n",
      " [-0.11624764  0.          0.          1.          0.        ]\n",
      " [ 0.46499055  0.70710678  0.          0.          1.        ]\n",
      " [ 1.62746694  1.41421356  1.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Definindo as transformações\n",
    "transformers = [\n",
    "    (\"num\", StandardScaler(), ['idade', 'salario']),\n",
    "    (\"cat\", OneHotEncoder(), ['departamento'])\n",
    "]\n",
    "\n",
    "column_trans = ColumnTransformer(transformers, remainder='passthrough')\n",
    "\n",
    "# Aplicando as transformações\n",
    "data_transformed = column_trans.fit_transform(df)\n",
    "\n",
    "print(data_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código acima:\n",
    "\n",
    "* Usamos `StandardScaler` para as colunas numéricas `idade` e `salario`.\n",
    "\n",
    "* Usamos `OneHotEncoder` para a coluna categórica `departamento`.\n",
    "\n",
    "* A opção `remainder='passthrough'` garante que qualquer coluna não especificada no transformador seja passada sem alterações. Se não quisermos incluir colunas não especificadas no resultado, podemos usar `remainder='drop'`.\n",
    "\n",
    "O resultado, `data_transformed`, é uma matriz numpy com colunas padronizadas e codificadas conforme especificado. Você pode facilmente integrar este `ColumnTransformer` em um `Pipeline` do scikit-learn para combinar pré-processamento e modelagem em etapas sequenciais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de um pipeline que aplica diferentes pré-processamentos a colunas numéricas e categóricas usando `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "df_titanic.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.78\n",
      "Precisão: 0.74\n",
      "Recall: 0.70\n",
      "F1 Score: 0.72\n"
     ]
    }
   ],
   "source": [
    "y = df_titanic[\"survived\"]\n",
    "X = df_titanic.drop(columns=[\"survived\"])\n",
    "\n",
    "# Divida os dados em conjuntos de treinamento e test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definindo as colunas\n",
    "num_features = ['age', 'fare']\n",
    "cat_features = ['sex', 'embarked']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # Preenche os valores faltantes com a média\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), num_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Acurácia: {accuracy:.2f}\")\n",
    "print(f\"Precisão: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex03: Aplicando a `ColumnTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere que temos um dataset que contém informações sobre carros. Os dados contêm duas características: `peso` (numérica) e `marca` (categórica). Seu objetivo é padronizar a característica `peso` e aplicar codificação one-hot na característica `marca` utilizando a `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peso</th>\n",
       "      <th>marca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "      <td>Toyota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1700</td>\n",
       "      <td>Ford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   peso   marca\n",
       "0  1500  Toyota\n",
       "1  1700    Ford"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'peso': [1500, 1700, 1200, 2000, 1600],\n",
    "    'marca': ['Toyota', 'Ford', 'Ford', 'Chevrolet', 'Toyota']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       peso  marca_Chevrolet  marca_Ford  marca_Toyota\n",
      "0 -0.383482              0.0         0.0           1.0\n",
      "1  0.383482              0.0         1.0           0.0\n",
      "2 -1.533930              0.0         1.0           0.0\n",
      "3  1.533930              1.0         0.0           0.0\n",
      "4  0.000000              0.0         0.0           1.0\n"
     ]
    }
   ],
   "source": [
    "# Definindo as transformações\n",
    "transformers = [\n",
    "    (\"num\", StandardScaler(), ['peso']),  # Padronizar a coluna 'peso'\n",
    "    (\"cat\", OneHotEncoder(), ['marca'])   # Codificação one-hot para a coluna 'marca'\n",
    "]\n",
    "\n",
    "column_trans = ColumnTransformer(transformers, remainder='passthrough')\n",
    "\n",
    "data_transformed = column_trans.fit_transform(df)\n",
    "\n",
    "# Para obter os nomes das colunas após a codificação one-hot\n",
    "columns_after_onehot = column_trans.named_transformers_['cat'].get_feature_names_out(['marca'])\n",
    "\n",
    "# Unir colunas e converter a matriz numpy para DataFrame\n",
    "df_transformed = pd.DataFrame(data_transformed, columns=['peso'] + list(columns_after_onehot))\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "\n",
    "<!--\n",
    "# Definindo as transformações\n",
    "transformers = [\n",
    "    (\"num\", StandardScaler(), ['peso']),  # Padronizar a coluna 'peso'\n",
    "    (\"cat\", OneHotEncoder(), ['marca'])   # Codificação one-hot para a coluna 'marca'\n",
    "]\n",
    "\n",
    "column_trans = ColumnTransformer(transformers, remainder='passthrough')\n",
    "\n",
    "data_transformed = column_trans.fit_transform(df)\n",
    "\n",
    "# Para obter os nomes das colunas após a codificação one-hot\n",
    "columns_after_onehot = column_trans.named_transformers_['cat'].get_feature_names_out(['marca'])\n",
    "\n",
    "# Unir colunas e converter a matriz numpy para DataFrame\n",
    "df_transformed = pd.DataFrame(data_transformed, columns=['peso'] + list(columns_after_onehot))\n",
    "print(df_transformed)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FunctionTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `FunctionTransformer` transforma dados de entrada (como um DataFrame) aplicando uma função fornecida pelo usuário. Basicamente, ele é uma maneira de converter qualquer função em um objeto transformador compatível com o scikit-learn para que possa ser usado em um `Pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos um pequeno conjunto de dados que contém informações sobre diferentes frutas: seu nome, peso e calorias por 100 gramas.\n",
    "\n",
    "Nosso objetivo é usar o `FunctionTransformer` para adicionar uma coluna que mostra as calorias totais da fruta com base em seu peso e nas calorias por 100 gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fruta</th>\n",
       "      <th>Peso (g)</th>\n",
       "      <th>Calorias por 100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maçã</td>\n",
       "      <td>150</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Banana</td>\n",
       "      <td>120</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fruta  Peso (g)  Calorias por 100g\n",
       "0    Maçã       150                 52\n",
       "1  Banana       120                 96"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Fruta': ['Maçã', 'Banana', 'Cereja', 'Uva'],\n",
    "    'Peso (g)': [150, 120, 5, 200],\n",
    "    'Calorias por 100g': [52, 96, 3, 69]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar uma função que calcula as calorias totais de cada fruta com base em seu peso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_calories(X):\n",
    "    X['Calorias Totais'] = (X['Peso (g)'] * X['Calorias por 100g']) / 100\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a função de transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fruta  Peso (g)  Calorias por 100g  Calorias Totais\n",
      "0    Maçã       150                 52            78.00\n",
      "1  Banana       120                 96           115.20\n",
      "2  Cereja         5                  3             0.15\n",
      "3     Uva       200                 69           138.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Criando o transformador\n",
    "total_calories_transformer = FunctionTransformer(add_total_calories)\n",
    "\n",
    "# Criando o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('add_calories', total_calories_transformer)\n",
    "])\n",
    "\n",
    "# Aplicando a transformação ao DataFrame\n",
    "df_transformed = pipeline.transform(df)\n",
    "\n",
    "print(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex04: Aplicando a `FunctionTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você possui um conjunto de dados sobre diferentes bebidas e a quantidade de açúcar que elas contêm por 100ml. Você quer determinar a quantidade total de açúcar em uma garrafa de bebida com base em seu volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Bebida': ['Coca-Cola', 'Suco de Laranja', 'Água Tônica', 'Ice Tea'],\n",
    "    'Volume (ml)': [330, 250, 200, 350],\n",
    "    'Açúcar por 100ml (g)': [10.6, 8.2, 7.0, 5.5]\n",
    "}\n",
    "\n",
    "df_bebidas = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um `Pipeline` que utiliza o `FunctionTransformer` para adicionar uma coluna ao conjunto de dados que mostra a quantidade total de açúcar em cada garrafa de bebida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "\n",
    "<!--\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def add_total_sugar(X):\n",
    "    X['Açúcar Total (g)'] = (X['Volume (ml)'] * X['Açúcar por 100ml (g)']) / 100\n",
    "    return X\n",
    "\n",
    "# Criando o transformador\n",
    "total_sugar_transformer = FunctionTransformer(add_total_sugar)\n",
    "\n",
    "# Criando o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('add_sugar', total_sugar_transformer)\n",
    "])\n",
    "\n",
    "# Aplicando a transformação ao DataFrame\n",
    "df_transformed = pipeline.transform(df_bebidas)\n",
    "\n",
    "print(df_transformed)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `GridSearchCV` realiza uma pesquisa exaustiva por meio de combinações de parâmetros especificadas para um estimador. Ele automatiza o processo de ajuste de parâmetros de um modelo para encontrar a combinação ótima que produz os melhores resultados de acordo com uma métrica de avaliação específica.\n",
    "\n",
    "\"GridSearch\" refere-se à abordagem de experimentar todas as possíveis combinações de parâmetros em uma grade predefinida. \"CV\" refere-se à validação cruzada (cross-validation), o método usado para avaliar o desempenho de cada combinação de parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhores parâmetros: {'classifier__C': 10, 'classifier__gamma': 0.1}\n",
      "Melhor pontuação de validação cruzada: 0.9731225296442687\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Carregar dados\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# Criar um pipeline simples\n",
    "pipe = Pipeline([('classifier', SVC())])\n",
    "\n",
    "# Parâmetros a serem testados\n",
    "param_grid = {\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Executar GridSearchCV\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Melhores parâmetros e score\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "print(\"Melhor pontuação de validação cruzada:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex05: Otimização de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que você está trabalhando em um projeto onde precisa prever o consumo médio de energia elétrica de uma casa com base em algumas características. Você possui um conjunto de dados de casas com suas características e o respectivo consumo médio de energia elétrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Área (m^2)</th>\n",
       "      <th>Número de Quartos</th>\n",
       "      <th>Idade da Casa (anos)</th>\n",
       "      <th>Consumo Médio de Energia (kWh/mês)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Área (m^2)  Número de Quartos  Idade da Casa (anos)  \\\n",
       "0          50                  1                     5   \n",
       "1          80                  2                    12   \n",
       "\n",
       "   Consumo Médio de Energia (kWh/mês)  \n",
       "0                                 150  \n",
       "1                                 220  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Área (m^2)': [50, 80, 120, 150, 200, 65, 85, 110, 95, 135],\n",
    "    'Número de Quartos': [1, 2, 4, 3, 4, 2, 2, 3, 2, 4],\n",
    "    'Idade da Casa (anos)': [5, 12, 8, 20, 25, 10, 12, 6, 15, 18],\n",
    "    'Consumo Médio de Energia (kWh/mês)': [150, 220, 370, 420, 540, 190, 230, 310, 280, 400]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando `Pipeline` e `GridSearchCV`, ajuste um modelo de regressão para prever o consumo médio de energia de uma casa. Teste diferentes valores de parâmetros para o regressor e escolha o que fornece o melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua solução aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "\n",
    "<!--\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('Consumo Médio de Energia (kWh/mês)', axis=1)\n",
    "y = df['Consumo Médio de Energia (kWh/mês)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'regressor__fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Melhores parâmetros:\", grid.best_params_)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Data Leakage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *data leakage* (vazamento de dados) refere-se a de preparação ou modelagem de dados, em que informações do conjunto de dados de teste \"vazam\" para o conjunto de treinamento. Como resultado, o modelo obtém um acesso antecipado, direta ou indiretamente, a dados que não deveriam ser usados na fase de treinamento. Esse vazamento pode levar a uma avaliação superotimista do desempenho do modelo, já que o modelo pode se sair bem em dados que ele já \"viu\" anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suponha que temos um conjunto de dados com uma única característica e queremos padronizá-la. Primeiro, vamos criar um conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um dataset fictício\n",
    "X = np.random.rand(100, 1) * 10  # 100 amostras, 1 característica\n",
    "y = 2.5 * X.squeeze() + 3 + np.random.randn(100) * 2  # relação linear com algum ruído"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abordagem errada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE com vazamento de dados: 2.6675358621798506\n"
     ]
    }
   ],
   "source": [
    "# Padronizar todo o conjunto de dados\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir em treino e teste\n",
    "X_train_leak, X_test_leak, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar e testar\n",
    "model = LinearRegression().fit(X_train_leak, y_train)\n",
    "y_pred = model.predict(X_test_leak)\n",
    "print(\"RMSE com vazamento de dados:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abordagem correta (usando Pipelines)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE usando pipeline: 2.6675358621798497\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construindo o pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Treinar e testar usando o pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"RMSE usando pipeline:\", np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex06: Evitando Vazamento de Dados com o Dataset Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto de dados Iris é um dos datasets mais famosos em machine learning e estatística. Consiste em 150 observações de 3 diferentes tipos de flores de íris (Setosa, Versicolour e Virginica). Cada observação possui quatro características: comprimento das sépalas, largura das sépalas, comprimento das pétalas e largura das pétalas.\n",
    "\n",
    "Suponha que as medidas das pétalas são tomadas por uma máquina diferente das medidas das sépalas e, por alguma razão, todas as medidas das pétalas acima de 2 cm têm um erro sistemático, fazendo com que sejam relatadas como 0.5 cm menores do que a medição real.\n",
    "\n",
    "Entretanto, você só descobriu isso após a coleta de dados. Portanto, precisa corrigir os dados de treinamento e verificar o impacto no modelo que já foi treinado.\n",
    "\n",
    "**TAREFA:**\n",
    "\n",
    "1. Carregue o dataset Iris.\n",
    "2. Divida o dataset em conjuntos de treinamento e teste.\n",
    "3. Treine um modelo de classificação (por exemplo, uma regressão logística ou SVM) usando todas as características.\n",
    "4. Avalie o desempenho do modelo no conjunto de teste e anote os resultados.\n",
    "5. Simule o \"vazamento de dados\" ajustando todas as medidas das pétalas no conjunto de treinamento que estão acima de 2 cm, diminuindo-as em 0.5 cm.\n",
    "6. Treine o modelo novamente usando os dados ajustados.\n",
    "7. Avalie o desempenho do modelo no conjunto de teste e compare com os resultados anteriores.\n",
    "8. Agora, corrija o vazamento de dados da seguinte forma:  \n",
    "   a. Aplique a correção em todo o dataset (treinamento e teste).  \n",
    "   b. Treine o modelo novamente.  \n",
    "   c. Avalie o desempenho e compare com os dois resultados anteriores.\n",
    "\n",
    "**OBJETIVO**  \n",
    "Este exercício visa ilustrar o impacto que o vazamento de dados pode ter no desempenho de um modelo e a importância de corrigir os dados de maneira adequada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy sem correção: 1.0000\n",
      "Accuracy com vazamento de dados: 0.8889\n",
      "Accuracy após correção: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Carregando o dataset Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Dividindo o dataset em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Treinando um modelo de classificação\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Avaliando o desempenho do modelo no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_original = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy sem correção: {acc_original:.4f}\")\n",
    "\n",
    "# 5. Simulando o \"vazamento de dados\"\n",
    "petal_length_column = 2  # Índice da coluna 'comprimento das pétalas' no dataset\n",
    "X_train[:, petal_length_column] = np.where(X_train[:, petal_length_column] > 2, X_train[:, petal_length_column] - 0.5, X_train[:, petal_length_column])\n",
    "\n",
    "# 6. Treinando o modelo novamente com os dados ajustados\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 7. Avaliando o desempenho do modelo ajustado no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_leaked = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy com vazamento de dados: {acc_leaked:.4f}\")\n",
    "\n",
    "# 8. Corrigindo o vazamento de dados\n",
    "X[:, petal_length_column] = np.where(X[:, petal_length_column] > 2, X[:, petal_length_column] + 0.5, X[:, petal_length_column])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_corrected = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy após correção: {acc_corrected:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clique aqui para ver a resposta*\n",
    "\n",
    "<!--\n",
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Carregando o dataset Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Dividindo o dataset em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Treinando um modelo de classificação\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Avaliando o desempenho do modelo no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_original = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy sem correção: {acc_original:.4f}\")\n",
    "\n",
    "# 5. Simulando o \"vazamento de dados\"\n",
    "petal_length_column = 2  # Índice da coluna 'comprimento das pétalas' no dataset\n",
    "X_train[:, petal_length_column] = np.where(X_train[:, petal_length_column] > 2, X_train[:, petal_length_column] - 0.5, X_train[:, petal_length_column])\n",
    "\n",
    "# 6. Treinando o modelo novamente com os dados ajustados\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 7. Avaliando o desempenho do modelo ajustado no conjunto de teste\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_leaked = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy com vazamento de dados: {acc_leaked:.4f}\")\n",
    "\n",
    "# 8. Corrigindo o vazamento de dados\n",
    "X[:, petal_length_column] = np.where(X[:, petal_length_column] > 2, X[:, petal_length_column] + 0.5, X[:, petal_length_column])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_corrected = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy após correção: {acc_corrected:.4f}\")\n",
    "\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
