{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost significa \"Extreme Gradient Boosting\". O XGBoost é um algoritmo de aprendizado de máquina popular e eficaz baseado na estrutura do \"gradient boosting\". A ideia básica do gradient boosting é combinar muitos modelos simples, conhecidos como \"weak learners\", para criar um conjunto robusto e altamente preciso, ou um \"strong learner\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXEMPLO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que você esteja tentando prever se um cliente de um banco vai ou não se inscrever para um novo produto de cartão de crédito (tarefa de classificação binária). Temos várias características para cada cliente. O objetivo poderia ser prever a coluna \"Produto de Cartão de Crédito\" com base nas outras colunas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ID | Idade | Salario Anual (R$) | Pontuação de Crédito | Cartão de Crédito Existente (Sim=1, Não=0) | Emprestimo Ativo (Sim=1, Não=0) | Produto de Cartão de Crédito (Sim=1, Não=0) |\n",
    "|-----------|-------|---------------------|-----------------------|----------------------------------------------|----------------------------------|----------------------------------------|\n",
    "| 1         | 35    | 70,000              | 670                   | 0                                            | 1                                | 0                                      |\n",
    "| 2         | 45    | 85,000              | 740                   | 1                                            | 0                                | 1                                      |\n",
    "| 3         | 50    | 120,000             | 710                   | 1                                            | 1                                | 1                                      |\n",
    "| 4         | 23    | 35,000              | 580                   | 0                                            | 0                                | 0                                      |\n",
    "| 5         | 44    | 75,000              | 700                   | 0                                            | 1                                | 0                                      |\n",
    "| 6         | 34    | 65,000              | 680                   | 1                                            | 1                                | 0                                      |\n",
    "| 7         | 36    | 90,000              | 730                   | 1                                            | 0                                | 1                                      |\n",
    "| 8         | 40    | 85,000              | 690                   | 0                                            | 1                                | 1                                      |\n",
    "| 9         | 29    | 45,000              | 650                   | 1                                            | 0                                | 0                                      |\n",
    "| 10        | 31    | 55,000              | 660                   | 0                                            | 1                                | 0                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algumas considerações sobre o XGBoost:\n",
    "\n",
    "1. **Regularização:** O XGBoost possui um parâmetro de regularização que pode ajudar a prevenir o overfitting. Essa é uma vantagem em relação a outros algoritmos de boosting.\n",
    "\n",
    "2. **Paralelização:** Enquanto o processo de boosting em si é sequencial (cada modelo é construído com base nos erros do anterior), o XGBoost consegue paralelizar a construção das árvores dentro de cada modelo, o que torna o algoritmo mais rápido.\n",
    "\n",
    "3. **Handling Missing Values:** O XGBoost pode lidar automaticamente com valores ausentes.\n",
    "\n",
    "4. **Flexibilidade:** Pode ser usado para problemas de classificação, regressão, ranking e personalização de funções de perda.\n",
    "\n",
    "5. **Tree Pruning:** Ao contrário do GBM (Gradient Boosting Machine), que para quando um número fixo de divisões é alcançado, o XGBoost cresce a árvore até um máximo especificado e então retrocede e remove as divisões além do qual não ocorre nenhum ganho positivo.\n",
    "\n",
    "6. **Built-in Cross-Validation:** O XGBoost permite que o usuário execute uma validação cruzada a cada iteração do processo de boosting e, assim, é fácil obter o número ideal de iterações de boosting em uma única execução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Algoritmo na Prática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instalação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.3/200.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jeff/anaconda3/lib/python3.8/site-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /home/jeff/anaconda3/lib/python3.8/site-packages (from xgboost) (1.7.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingestão**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('caminho-para-o-dataset.csv')\n",
    "\n",
    "# Dividir o DataFrame em features (X) e target (y)\n",
    "X = df[['Idade', 'Salario Anual (R$)', 'Pontuação de Crédito', 'Cartão de Crédito Existente', 'Emprestimo Ativo']]\n",
    "y = df['Produto de Cartão de Crédito']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treinamento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma instância do modelo\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Acurácia: %.2f%%\" % (accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
